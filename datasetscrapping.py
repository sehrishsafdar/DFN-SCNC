# -*- coding: utf-8 -*-
"""datasetscrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t4WS1IJ-ocp2gXBuiY6EyUTC_Lw41EGR
"""

from bs4 import BeautifulSoup
import pandas as pd
import requests
import csv
from datetime import datetime
dates = []
authors = []
statements = []
sources = []
targets = []
links = []
source_urls = []

def parse_date(date_str):
    try:
        return datetime.strptime(date_str, "%b %d %Y")
    except ValueError:
        try:
            date_str = date_str.replace('â€¢', '').strip()
            return datetime.strptime(date_str, "%B %d,")
        except ValueError:
            return None

def scrape_website(page_number):

    page_num = str(page_number)

    URL = 'https://www.politifact.com/factchecks/list/?page=' + page_num


    webpage = requests.get(URL)
    soup = BeautifulSoup(webpage.text, "html.parser")

    statement_footer = soup.find_all('footer', attrs={'class': 'm-statement__footer'})
    statement_quote = soup.find_all('div', attrs={'class': 'm-statement__quote'})
    statement_meta = soup.find_all('div', attrs={'class': 'm-statement__meta'})
    target = soup.find_all('div', attrs={'class': 'm-statement__meter'})

    if not statement_footer:
        return False

    for i in range(len(statement_footer)):
        link1 = statement_footer[i].text.strip()
        name_and_date = link1.split()
        if len(name_and_date) < 7:
            continue
        first_name = name_and_date[1]
        last_name = name_and_date[2]
        full_name = first_name + ' ' + last_name
        month = name_and_date[4]
        day = name_and_date[5].replace(',', '')
        year = name_and_date[6] if len(name_and_date) > 6 else str(datetime.now().year)

        date_str = f"{month} {day} {year}"
        date_obj = parse_date(date_str)


        if date_obj is None or not (datetime(2014, 1, 1) <= date_obj <= datetime(2024, 12, 31)):
            continue

        dates.append(date_obj.strftime("%Y-%m-%d"))
        authors.append(full_name)

        link2 = statement_quote[i].find_all('a')
        statements.append(link2[0].text.strip())
        post_link = 'https://www.politifact.com' + link2[0]['href']
        links.append(post_link)

        link3 = statement_meta[i].find_all('a')
        source_text = link3[0].text.strip()
        sources.append(source_text)

        if any(platform in source_text.lower() for platform in ['X']):

            fact = target[i].find('div', attrs={'class':'c-image'}).find('img').get('alt')
            targets.append(fact)

            extracted_urls = extract_source_urls(post_link)
            source_urls.append(', '.join(extracted_urls))
        else:
            dates.pop()
            authors.pop()
            statements.pop()
            sources.pop()
            links.pop()

    return True

def extract_source_urls(url):
    try:
        webpage = requests.get(url)
        soup = BeautifulSoup(webpage.text, "html.parser")
        sources_section = soup.find('section', id='sources')
        source_links = sources_section.find_all('a', href=True) if sources_section else []
        return [link['href'] for link in source_links]
    except Exception as e:
        print(f"Error processing URL {url}: {e}")
        return []

def save_to_csv(data, filename='politifact_data.csv'):

    dates, authors, statements, sources, targets, links, source_urls = data


    min_length = min(len(dates), len(authors), len(statements), len(sources), len(targets), len(links), len(source_urls))
    dates = dates[:min_length]
    authors = authors[:min_length]
    statements = statements[:min_length]
    sources = sources[:min_length]
    targets = targets[:min_length]
    links = links[:min_length]
    source_urls = source_urls[:min_length]


    with open(filename, 'w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['Date', 'Author', 'Statement', 'Source', 'Target', 'Link', 'Source URLs'])  # Write the header row
        for row in zip(dates, authors, statements, sources, targets, links, source_urls):
            writer.writerow(row)


page_number = 1
while True:
    if not scrape_website(page_number):
        break
    page_number += 1


data = pd.DataFrame({
    'Date': dates,
    'Author': authors,
    'Statement': statements,
    'Source': sources,
    'Target': targets,
    'Link': links,
    'Source URLs': source_urls
})


save_to_csv((dates, authors, statements, sources, targets, links, source_urls))


print(data)


def getBinaryNumTarget(text):
    return 1 if text.lower() == 'true' else 0

def getBinaryTarget(text):
    return 'REAL' if text.lower() == 'true' else 'FAKE'

data['BinaryTarget'] = data['Target'].apply(getBinaryTarget)
data['BinaryNumTarget'] = data['Target'].apply(getBinaryNumTarget)


data.to_csv('politifact_data_with_binary_targets.csv', index=False)